{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np;\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def normal_std(x):\n",
    "    return x.std() * np.sqrt((len(x) - 1.) / (len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_utility(object):\n",
    "    # train and valid is the ratio of training set and validation set. test = 1 - train - valid\n",
    "    def __init__(self, file_name, train, valid, cuda, normalize=2, y_label=0):\n",
    "        self.cuda = cuda;\n",
    "        fin = open(file_name);\n",
    "        self.rawdat = np.loadtxt(fin, delimiter=',');\n",
    "        self.P = 12  # RNN长度\n",
    "        self.h = 1  # 预测未来h期\n",
    "        self.dat = np.zeros(self.rawdat.shape)  # 标准化后的数据\n",
    "        self.n, self.m = self.dat.shape;\n",
    "        self.tickers = 10\n",
    "        self.n_split = int(self.n/self.tickers)\n",
    "        self.y_label = y_label\n",
    "#         if y_label: self.m -= y_label\n",
    "        self.normalize = normalize\n",
    "        self.scale = np.ones(self.m)\n",
    "        self.scale_add = np.zeros(self.m)\n",
    "        self.train_percentage = train\n",
    "        self._normalized(normalize)\n",
    "        \n",
    "        \n",
    "        self._split(int(train * self.n_split), int((train + valid) * self.n_split), self.n_split)\n",
    "\n",
    "        self.scale = torch.from_numpy(self.scale).float();\n",
    "\n",
    "        if self.cuda:\n",
    "            self.scale = self.scale.cuda()\n",
    "        self.scale = Variable(self.scale)\n",
    "\n",
    "    def _normalized(self, normalize):\n",
    "        # no normalize\n",
    "        \n",
    "        self.train_norm = self.rawdat[:int(self.train_percentage * self.n_split)] # 用训练集来做标准化\n",
    "        for t in range(1,self.tickers):\n",
    "#             test11 = np.array(self.rawdat)[t:int(self.train_percentage * self.n_split)+t]\n",
    "#             print(type(self.train_norm),type(self.rawdat))\n",
    "#             print(self.train_norm.shape,self.rawdat[t*self.n_split:int(self.train_percentage * self.n_split)+t*self.n_split].shape)\n",
    "            self.train_norm = np.concatenate((self.train_norm,\\\n",
    "                                              self.rawdat[t*self.n_split:int(self.train_percentage * self.n_split)+t*self.n_split]),0)\n",
    "        if (normalize == 0):\n",
    "            self.dat = self.rawdat\n",
    "        print(self.train_norm.shape)\n",
    "        #  normlized by the MinMaxScalar\n",
    "        if (normalize == 1):\n",
    "            for i in range(self.m):\n",
    "                # self.scale[i] = np.max(np.abs(self.rawdat[:, i]))\n",
    "                if (np.max(self.train_norm[:, i]) - np.min(self.train_norm[:, i])) != 0:\n",
    "#                     self.dat[:, i] = (self.rawdat[:, i] - np.min(self.rawdat[:, i])) / \\\n",
    "#                                      (np.max(self.rawdat[:, i]) - np.min(self.rawdat[:, i]))\n",
    "                    self.scale[i] = np.max(self.train_norm[:, i]) - np.min(self.train_norm[:, i])\n",
    "                    self.scale_add[i] = np.min(self.train_norm[:, i])\n",
    "                self.dat[:, i] = (self.rawdat[:, i]-self.scale_add[i])/self.scale[i]\n",
    "\n",
    "        # normlized by the maximum value of each row(sensor).\n",
    "        if (normalize == 2): \n",
    "            for i in range(self.m):\n",
    "                self.scale[i] = np.max(np.abs(self.rawdat[:, i]));\n",
    "                self.dat[:, i] = self.rawdat[:, i] / np.max(np.abs(self.rawdat[:, i]));\n",
    "\n",
    "#         if self.y_label:\n",
    "#             self.dat[:, -1] = self.rawdat[:, -1]\n",
    "\n",
    "    def _split(self, train, valid, test):\n",
    "\n",
    "        train_set = range(self.P, train);\n",
    "        valid_set = range(train, valid);\n",
    "        test_set = range(valid, self.n_split);\n",
    "        self.train = self._batchify(train_set, self.h);\n",
    "        self.valid = self._batchify(valid_set, self.h);\n",
    "        self.test = self._batchify(test_set, self.h);\n",
    "\n",
    "\n",
    "    def _batchify(self, idx_set, horizon):\n",
    "        n = len(idx_set)\n",
    "        X = torch.zeros((n*self.tickers, self.P, self.m-self.y_label))  # 资料数量,RNN长度,特征数\n",
    "        Y = torch.zeros((n*self.tickers, max(1, self.y_label)))\n",
    "        \n",
    "        # threshold for classifcation problem\n",
    "        temp = sorted(self.dat[:,-1])\n",
    "        bottom, top = temp[len(temp)//3], temp[len(temp)*2//3]\n",
    "        for c in range(self.tickers):\n",
    "            for i in range(n):\n",
    "                end = idx_set[i] +c*self.n_split - self.h + 1\n",
    "                start = end - self.P\n",
    "                \n",
    "                if self.y_label:\n",
    "                    X[i+c*n, :, :] = torch.from_numpy(self.dat[start+1:end+1, :-self.y_label])\n",
    "                    #Y[i+c*n, :] = torch.from_numpy(self.dat[end, -self.y_label:])  # regression\n",
    "                    \n",
    "                    # classification\n",
    "                    class_value = np.where(self.dat[end, -self.y_label:]>=top,2,np.where(self.dat[end, -self.y_label:]<=bottom,0,1))\n",
    "#                     class_onehot = np.zeros([class_value.size, 3])\n",
    "#                     class_onehot[np.arange(class_value.size), class_value.reshape(1, class_value.size)]  = 1\n",
    "#                     print(class_value,Y.shape)\n",
    "                    Y[i+c*n, :] = torch.from_numpy(class_value)  \n",
    "                else:\n",
    "                    X[i+c*n, :, :] = torch.from_numpy(self.dat[start:end, :])\n",
    "                    Y[i+c*n, :] = torch.from_numpy(self.dat[idx_set[i], 0:1])\n",
    "        return [X, Y]\n",
    "\n",
    "    def get_batches(self, inputs, targets, batch_size, shuffle=True):\n",
    "        length = len(inputs)\n",
    "        if shuffle:\n",
    "            index = torch.randperm(length)\n",
    "        else:\n",
    "            index = torch.LongTensor(range(length))\n",
    "        start_idx = 0\n",
    "        while (start_idx < length):\n",
    "            end_idx = min(length, start_idx + batch_size)\n",
    "            excerpt = index[start_idx:end_idx]\n",
    "            X = inputs[excerpt]\n",
    "            Y = targets[excerpt]\n",
    "            if (self.cuda):\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()\n",
    "            yield Variable(X), Variable(Y)\n",
    "            start_idx += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630, 57)\n",
      "torch.Size([510, 12, 56]) torch.Size([510, 1])\n",
      "tensor([[0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "[[0.55844525 0.44771718 0.67662829 ... 0.39409162 0.41554599 0.52520019]\n",
      " [0.52520019 0.36216231 0.60059489 ... 0.38859079 0.47538595 0.52875616]\n",
      " [0.52875616 0.37294787 0.58095756 ... 0.37201633 0.50033872 0.5476282 ]\n",
      " ...\n",
      " [0.46834086 0.37356049 0.50632402 ... 0.45555821 0.41092351 0.60176133]\n",
      " [0.60176133 0.3701305  0.52753316 ... 0.28247658 0.4995578  0.50338271]\n",
      " [0.50338271 0.39094623 0.53994567 ... 0.51011212 0.4233324  0.52735578]] (1060, 57) torch.Size([57])\n"
     ]
    }
   ],
   "source": [
    "Data = Data_utility('./data/stock.txt', train=0.6, valid=0.2, cuda=True, normalize=1, y_label=1)\n",
    "\n",
    "print(Data.train[0].shape,Data.train[1].shape)\n",
    "# print(Data.valid[0])\n",
    "print(Data.train[1])\n",
    "print(Data.dat,Data.dat.shape,Data.scale.shape)\n",
    "# print(Data.scale,Data.scale_add)\n",
    "# print(Data.valid[0][:,0]*Data.scale[0]+Data.scale_add[0],Data.valid[1]*Data.scale[-1]+Data.scale_add[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 1]) tensor([[-2.2899, -1.9268, -0.0192,  1.8996, -0.0977],\n",
      "        [ 0.3105, -0.8041, -0.2124, -0.5968, -0.5938],\n",
      "        [-0.3112,  0.5530, -0.0460,  1.7073,  0.8941]], requires_grad=True) tensor(2.6304, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(target,input,output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
